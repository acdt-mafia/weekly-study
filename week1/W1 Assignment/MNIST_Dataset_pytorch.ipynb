{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a07ae665",
        "outputId": "9b6c9a27-3f86-4e9c-9192-22d1ab2fd8a4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set device to GPU if available, otherwise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalize-> 이미지 정규화. (x-평균)을 표준편차로 나늠. 수렴 속도 향상.데이터가 특정 분포를 따르게 해 모델이 잘 작동되도록 함."
      ],
      "metadata": {
        "id": "uvAN9VpXtM1R"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "898992be",
        "outputId": "7643a28c-c877-4c48-c796-4825e78c3352"
      },
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Convert image to PyTorch Tensor\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # Normalize pixel values\n",
        "])\n",
        "\n",
        "# Download and load training data\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load test data\n",
        "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of test samples: {len(test_dataset)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 52.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.76MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 15.0MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.93MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 60000\n",
            "Number of test samples: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2개 hidden layer 활성화 함수는 ReLU인 MLP임.  init-> 객체가 생성될 때 자동으로 호출. pytorch는 nn,module 생성자 호출.\n",
        "평탄화? 한번에 처리되는 이미지 개수-> batch size, mnist는 채널이 한개, 배치 크기 동일. 1차원 벡터로 픽셀 이미지 펼쳤을 때 784개의 픽셀 값\n",
        "x.view->텐서 형태 변경nit-> 객체가 생성될 때 자동으로 호출. pytorch는 nn,module 생성자 호출. 그 nn이 torch.nn약어임. nn.ReLU-> torch.nn 모듈 안에 있는 클래스들임.\n",
        "평탄화? 한번에 처리되는 이미지 개수-> batch size, mnist는 채널이 한개, 배치 크기 동일. 1차원 벡터로 픽셀 이미지 펼쳤을 때 784개의 픽셀 값\n",
        "x.view-> 이거 개헷갈리노...x를 받아서 .view로 텐서의 형태(수치 형태?)를 변경하는거라는데 그리고 -1인자가 자동추론하고 self.이걸로 연산하고 다시 x가 나오는듯. in feature: 이전 레이어의 out feature랑 똑같음. 레이어로 들어오는 특징의 수. 맨 마지막 out feature 10개인 이유가 0부터 9까지 숫자.bias=true-> bias 여부임."
      ],
      "metadata": {
        "id": "0EvrUC9mtwQw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8e00a8e",
        "outputId": "7d55328a-7002-4ae0-caaf-d80e73b72965"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        # MNIST images are 28x28, so 28*28 = 784 input features\n",
        "        self.fc1 = nn.Linear(28 * 28, 512) # Input layer to first hidden layer\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, 256)   # First hidden layer to second hidden layer\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(256, 10)    # Second hidden layer to output layer (10 classes for digits 0-9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input image (batch_size, 1, 28, 28) to (batch_size, 784)\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model and move it to the device\n",
        "model = MLP().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e59c344"
      },
      "source": [
        "criterion-> lossfunction정의. nn.Cross어쩌구 이게 다중 클래스 분류 문제 일반적인 손실함수.optimizer는 최적화 도구 정의. 최적화 도구가 가중치 어케 엄데이트할지 결정하는 알고리즘임. Adam은 Adaptive model Estimation. lr은 learning rate. 한번 업데이트마다 모델 파라미터 얼마나 크게 변경할지 결정하는 값. epoch-> 모든 학습 데이터셋 통과한 횟수. running_loss-> 각 배치의 손실.  running_loss += loss.item()이걸로 계속 더해짐. 배치 100개마다 running loss 나타냄. 초기화해서 배치별 평균 손실 예상."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79af9a37",
        "outputId": "43558c21-1b1b-4e20-fc26-9a0197ea2ffa"
      },
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 5\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # 1. Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. Forward pass\n",
        "        outputs = model(data)\n",
        "\n",
        "        # 3. Calculate loss\n",
        "        loss = criterion(outputs, target)\n",
        "\n",
        "        # 4. Backward pass (compute gradients)\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0: # Print every 100 batches\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] finished. Average Loss: {running_loss / (len(train_loader) % 100 if len(train_loader) % 100 != 0 else 100):.4f}\")\n",
        "print(\"Training finished!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch [1/5], Batch [100/938], Loss: 0.5549\n",
            "Epoch [1/5], Batch [200/938], Loss: 0.2369\n",
            "Epoch [1/5], Batch [300/938], Loss: 0.2219\n",
            "Epoch [1/5], Batch [400/938], Loss: 0.1791\n",
            "Epoch [1/5], Batch [500/938], Loss: 0.1601\n",
            "Epoch [1/5], Batch [600/938], Loss: 0.1531\n",
            "Epoch [1/5], Batch [700/938], Loss: 0.1334\n",
            "Epoch [1/5], Batch [800/938], Loss: 0.1367\n",
            "Epoch [1/5], Batch [900/938], Loss: 0.1084\n",
            "Epoch [1/5] finished. Average Loss: 0.1174\n",
            "Epoch [2/5], Batch [100/938], Loss: 0.0910\n",
            "Epoch [2/5], Batch [200/938], Loss: 0.0822\n",
            "Epoch [2/5], Batch [300/938], Loss: 0.0933\n",
            "Epoch [2/5], Batch [400/938], Loss: 0.0917\n",
            "Epoch [2/5], Batch [500/938], Loss: 0.0881\n",
            "Epoch [2/5], Batch [600/938], Loss: 0.0840\n",
            "Epoch [2/5], Batch [700/938], Loss: 0.0798\n",
            "Epoch [2/5], Batch [800/938], Loss: 0.0868\n",
            "Epoch [2/5], Batch [900/938], Loss: 0.0841\n",
            "Epoch [2/5] finished. Average Loss: 0.0726\n",
            "Epoch [3/5], Batch [100/938], Loss: 0.0531\n",
            "Epoch [3/5], Batch [200/938], Loss: 0.0666\n",
            "Epoch [3/5], Batch [300/938], Loss: 0.0496\n",
            "Epoch [3/5], Batch [400/938], Loss: 0.0634\n",
            "Epoch [3/5], Batch [500/938], Loss: 0.0654\n",
            "Epoch [3/5], Batch [600/938], Loss: 0.0627\n",
            "Epoch [3/5], Batch [700/938], Loss: 0.0674\n",
            "Epoch [3/5], Batch [800/938], Loss: 0.0602\n",
            "Epoch [3/5], Batch [900/938], Loss: 0.0608\n",
            "Epoch [3/5] finished. Average Loss: 0.0690\n",
            "Epoch [4/5], Batch [100/938], Loss: 0.0443\n",
            "Epoch [4/5], Batch [200/938], Loss: 0.0344\n",
            "Epoch [4/5], Batch [300/938], Loss: 0.0534\n",
            "Epoch [4/5], Batch [400/938], Loss: 0.0372\n",
            "Epoch [4/5], Batch [500/938], Loss: 0.0516\n",
            "Epoch [4/5], Batch [600/938], Loss: 0.0487\n",
            "Epoch [4/5], Batch [700/938], Loss: 0.0515\n",
            "Epoch [4/5], Batch [800/938], Loss: 0.0467\n",
            "Epoch [4/5], Batch [900/938], Loss: 0.0552\n",
            "Epoch [4/5] finished. Average Loss: 0.0475\n",
            "Epoch [5/5], Batch [100/938], Loss: 0.0381\n",
            "Epoch [5/5], Batch [200/938], Loss: 0.0270\n",
            "Epoch [5/5], Batch [300/938], Loss: 0.0312\n",
            "Epoch [5/5], Batch [400/938], Loss: 0.0308\n",
            "Epoch [5/5], Batch [500/938], Loss: 0.0410\n",
            "Epoch [5/5], Batch [600/938], Loss: 0.0325\n",
            "Epoch [5/5], Batch [700/938], Loss: 0.0448\n",
            "Epoch [5/5], Batch [800/938], Loss: 0.0405\n",
            "Epoch [5/5], Batch [900/938], Loss: 0.0410\n",
            "Epoch [5/5] finished. Average Loss: 0.0580\n",
            "Training finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9bac07b"
      },
      "source": [
        "### 5. Evaluate the Model\n",
        "\n",
        "Finally, let's evaluate the trained model on the test dataset to see its performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aaec127",
        "outputId": "8635bc82-74a4-4b5b-ab05-42800f14a60f"
      },
      "source": [
        "model.eval() # Set the model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1) # Get the index of the max log-probability\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the model on the 10000 test images: {accuracy:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the 10000 test images: 97.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ｍｏｄｅｌ．ｅｖａｌ（）이건 ｐｙｔｏｒｃｈ 모델을 평가모드로 전환． ｍｏｄｅｌ．ｔｒａｉｎ이건 모델을 학습모드로 전환． ｅｖａｌ에서는 모든 뉴런을 확용해야됨． 그리고 ｎｏｍａｌｉｚａｔｏｎ이것도 멈춤， 일관된 정규화 적용해야되서．"
      ],
      "metadata": {
        "id": "QHy_nd4hf6PH"
      }
    }
  ]
}