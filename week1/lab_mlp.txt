노드 수가 많으면 무조건 좋은 걸까? 
노드 10개 레이어 4개일 때는 직선으로 떠서 실패했는데 또 노드를 100개로 늘려보니까 
완전 이차함수 형태로 잘 나옴. (sigmoid)

-> 노드 수가 많을수록 표현력(모델 용량, capacity)은 커지지만, 학습이 쉬워지거나 항상 
성능이 좋아지는 건 아니다. 

10 노드 x 4레이어가 직선 처럼 나온 건 표현력 부족이 아니라 학습이 제대로 안 된 상태. 
깊은 구조 + 작은 폭이라서 정보가 사라진 것이다. 
각 층마다 선형변환, activation, gradient 전달 과정이 일어나는데 폭(노드 수)이 작으면
gradient 소실(0된 거 못살림) 같은 거 때문에 단순한 함수(직선)에 수렴 가능
그런데 노드 수를 크게 늘리면 이러한 문제가 완화되어(일부 노두 죽어도 다른 노드가 보완)
비선형 구조가 성공적으로 학습될 수 있다. 

그런데 노드 수를 늘리면 파라미터(모델이 학습을 통해 바꾸는 숫자들, 가중치와 편향) 둘 다 
늘어난다. 그리고 파라미터가 많아지면 과적합 가능성 증가함(특히 데이터 적을 때). 함수 표현
능력이 뛰어나서 데이터에 딱맞는 복잡한 곡선을 만들 수도 있기 때문. 
그래서 적당히 해야 한다.  

generalization 일반화
학습하지 않은 새로운 데이터에 대해서도 잘 맞는 성능

regularization 정규화
= 과적합을 막는 기술
ex. L2 정규화, 드롭아웃, early stopping... 


결론
- 노드 수만 늘리는 건 capacity는 올라간다. 학습 잘되면 곡선 형태가 잘 나옴
- 레이어 깊이만 늘리는 것도 capacity 올라감. 하지만 gradient vanishing 조심
- 노드 x 레이어 둘 다 많으면 capacity 매우 커서 과적합 가능성도 높아짐. 
- 그래서 이를 막는 regularization이 존재하나봄
